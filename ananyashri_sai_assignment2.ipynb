{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asai2094/ds-3001/blob/main/ananyashri_sai_assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13ad028b-72b7-43ed-aa78-96fd4e518040",
      "metadata": {
        "id": "13ad028b-72b7-43ed-aa78-96fd4e518040"
      },
      "source": [
        "# Assignment: Data Wrangling and Exploratory Data Analysis\n",
        "## Do Q1 and Q2, and one other question.\n",
        "`! git clone https://www.github.com/asai2094/assignment2`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://www.github.com/asai2094/assignment2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOa-JeuZRGGg",
        "outputId": "2a422a4d-4b7e-4b63-e61c-5ad2bd43fc8d"
      },
      "id": "iOa-JeuZRGGg",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'assignment2' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5735a4d4-8be8-433a-a351-70eb8002e632",
      "metadata": {
        "id": "5735a4d4-8be8-433a-a351-70eb8002e632"
      },
      "source": [
        "**Q1.** Open the \"tidy_data.pdf\" document in the repo, which is a paper called Tidy Data by Hadley Wickham.\n",
        "\n",
        "  1. Read the abstract. What is this paper about?\n",
        "    \n",
        "    This paper is about data tidying and its significance in the process of data cleaning. It talks about the advantages of using data tidying - easier to clean up messy datasets, and develop tidy tools for data analysis.\n",
        "  2. Read the introduction. What is the \"tidy data standard\" intended to accomplish?\n",
        "\n",
        "    The \"tidy data standard\" is intended to provide a consistent means of organizing data values within a dataset. It was designed to make the initial exploration and analysis easier as well as to simplify the development of data analysis tools (those that work together). These tools enable data scientists to focus on domain specific problems as opposed to boring logistics.\n",
        "  3. Read the intro to section 2. What does this sentence mean: \"Like families, tidy datasets are all alike but every messy dataset is messy in its own way.\" What does this sentence mean: \"For a given dataset, it’s usually easy to figure out what are observations and what are variables, but it is surprisingly difficult to precisely define variables and observations in general.\"\n",
        "\n",
        "    The first sentence indicates that tidy datasets have base characteristics that can help us see that they conform to a certain consistent structure (they have identifiable features that are predictable and consistent amongst them all). Whereas messy datasets are messy in their own way - there is no predictability or consistency in how they are different. They deviate from the \"standard\" in many different ways.\n",
        "\n",
        "    The second sentence means that its sometimes easier to be able to distinguish between what are observations and what are variables for a particular dataset. When given a particular example, it can be almost intuitive to us to be able spot the differences. However, when you are trying to take that example away and try to describe them in general, the sentence mentions that its harder for us to wrap our heads around what that means abstractly.\n",
        "\n",
        "  4. Read Section 2.2. How does Wickham define values, variables, and observations?\n",
        "\n",
        "    Wickham defines values are what makes up a dataset. They are usually numbers or strings, depending on if they are quantitiative or qualitative.\n",
        "\n",
        "    He categorizes values as being a part of variables and observations. He defines variables as a collection of all values that represent the same underlying property over various units (i.e., height, temperature, time, etc.). He defines observations as all values that, across different attributes, that are measured on the same unit (i.e., people or race). He defines both variables and observations are easier to define usually when given a specific dataset as opposed to general observations.\n",
        "  5. How is \"Tidy Data\" defined in section 2.3?\n",
        "\n",
        "    \"Tidy Data\" is defined to have each variable make up a column, each observation make up a row, and each kind of observational unit make up a table. In other words, tiny data aligns the meaning of the dataset with its structure (to make sure its consistent). As the layout also ensures that different variables' values are always paired to the same observation, this is another example of how Tidy Data is easily suited to pull/access necessary variables (the standard way of accessing a variable).\n",
        "  6. Read the intro to Section 3 and Section 3.1. What are the 5 most common problems with messy datasets? Why are the data in Table 4 messy? What is \"melting\" a dataset?\n",
        "\n",
        "    The 5 most common problems with messy datasets are:\n",
        "    - Column headers are values, not variable names\n",
        "    - Multiple variables are stored in one column\n",
        "    - Variables are stored in both rows and columns\n",
        "    - Multiple types of observational units are stored in the same table\n",
        "    - A single observational unit is stored in multiple tables.\n",
        "\n",
        "    The data in Table 4 are messy because variables form both the rows and columns, and column headers are values, not variable names. This is one of the most common problems with messy datasetes, hence why Table 4 is categorized in such a way.\n",
        "    Melting a dataset means to turn columns into rows. Typically, you supply pre-existing columns (referred to as \"colvars\"), and the remaining columns are split into two new variables: one for the old column headings and one for the old column values.\n",
        "\n",
        "  7. Why, specifically, is table 11 messy but table 12 tidy and \"molten\"?\n",
        "  \n",
        "    Table 11 is messy because variables are stored in both rows and columns. Specifically, Table 11 has variables spread out in the following ways: individual columns (i.e., id, year, month, etc.), spread across columns (day, d1–d31), and across rows (min and max temperature). There are structural missing values for months with less than 31 days [specifically the last days of the month]. And on top of that, the element column stores names of variables, but is not a variable itself. There are columns in this dataset that represent each day of the month.\n",
        "\n",
        "    On the otherhand, Table 12 is tidy and \"molten\" because it adheres to the properties of a tidy dataset (one variable in each column and each row has a day's observations). More specifically, Table 12 has dropped the missing values of Table 11, melted Table 11 with colvars (id, year, month )and element (in which the end result of this makes it molten), and then further rotating the element variable back into the columns makes it tidy again.\n",
        "  8. Read Section 6. What is the \"chicken-and-egg\" problem with focusing on tidy data? What does Wickham hope happens in the future with further work on the subject of data wrangling?\n",
        "\n",
        "    The \"chicken-and-egg\" problem with focusing on tidy data is the difficulty between the interdependence of tiny data and the tools designed for it. It is difficult to update one without the other and it can potentially lock users in inefficient workflows (in other words, making them stuck in a local maxima where they can't improve their workflow).\n",
        "\n",
        "    Wickham hopes that the future with data wrangling, with further work on the subject, can enhance the current tidy data framework we currently have. One suggestion she makes is in order for us to understand the cognitive ascpects of data analysis, to incoporate insights from human-computer interactions field (i.e.,user-testing, ethnography, talk-aloud protocols) to develop more appropriate tools. Wickham also hopes to investigate alternate data formats like multidimensional arrays or other tools (i.e. choosing between array-tidy vs data-frame tidy) to optimize better performance and memory. Last but not the least, she hopes to develop other frameworks that can make tasks other than tidying in data cleaning easier.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072",
      "metadata": {
        "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072"
      },
      "source": [
        "**Q2.** This question provides some practice cleaning variables which have common problems.\n",
        "1. Numeric variable: For `./data/airbnb_hw.csv`, clean the `Price` variable as well as you can, and explain the choices you make. How many missing values do you end up with? (Hint: What happens to the formatting when a price goes over 999 dollars, say from 675 to 1,112?)\n",
        "2. Categorical variable: For the `./data/sharks.csv` data covered in the lecture, clean the \"Type\" variable as well as you can, and explain the choices you make.\n",
        "3. Dummy variable: For the pretrial data covered in the lecture, clean the `WhetherDefendantWasReleasedPretrial` variable as well as you can, and, in particular, replace missing values with `np.nan`.\n",
        "4. Missing values, not at random: For the pretrial data covered in the lecture, clean the `ImposedSentenceAllChargeInContactEvent` variable as well as you can, and explain the choices you make. (Hint: Look at the `SentenceTypeAllChargesAtConvictionInContactEvent` variable.)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # Import the numpy package into your workspace\n",
        "import pandas as pd  # Import the pandas package into your workspace"
      ],
      "metadata": {
        "id": "LV16-GQRI5S_"
      },
      "id": "LV16-GQRI5S_",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 2 part 1:\n",
        "df = pd.read_csv('./assignment2/data/airbnb_hw.csv',low_memory=False)\n",
        "df.head()\n",
        "df[\"Price\"].value_counts()\n",
        "df[\"Price\"].unique()\n",
        "df['Price'] = df['Price'].astype(str)# from https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwiCzJKJptiBAxUAFVkFHeErDfQQFnoECA4QAw&url=https%3A%2F%2Fsaturncloud.io%2Fblog%2Fpython-pandas-converting-object-to-string-type-in-dataframes%2F%23%3A~%3Atext%3DTo%2520convert%2520column%2520%27B%27%2520from%2Cthe%2520data%2520to%2520string%2520type.&usg=AOvVaw3YbN7WrYztatPu1rFAWcIw&opi=89978449\n",
        "df['Price'] = df[\"Price\"].str.replace(',', '')\n",
        "df['Price'] = pd.to_numeric(df[\"Price\"])\n",
        "df[\"Price\"].unique()\n",
        "\n",
        "df[\"Price\"+'_nan'] = df[\"Price\"].isnull()\n",
        "print('Total Missings: \\n', sum(df[\"Price\"+'_nan']),'\\n')\n",
        "\n",
        "#For the Price variable, I first looked at all the unique values in the column to get a sense of what was going on.\n",
        "#When doing this, I noticed that all the values were Strings and that numbers above 999 had commas in them.\n",
        "#So I converted everything into a string, replaced all the commas with nothing (like not a space or anything).\n",
        "#Then i converted all the values back to numerics. All of the changes were stored into df[\"Price\"] to store them.\n",
        "#I also made sure to keep track of any missings, so I created a dummy missing variable and calculating how many were missing\n",
        "#Returning that variable let me know that there were 0 missings in the data!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4di0bQEJ7mc",
        "outputId": "06cbb8a3-8ac2-4e43-978f-ce232808dc2b"
      },
      "id": "t4di0bQEJ7mc",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Missings: \n",
            " 0 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 2 part 2:\n",
        "df = pd.read_csv('./assignment2/data/sharks.csv',low_memory=False)\n",
        "df.head()\n",
        "df.describe()\n",
        "#df.loc[]\n",
        "df[\"Type\"].value_counts()\n",
        "df[\"Type\"].unique()\n",
        "df['Type'] = df['Type'].replace({\n",
        "    'Boat': 'Watercraft',\n",
        "    'Boating': 'Watercraft',\n",
        "    'Boatomg': 'Watercraft',  #consolidating all into Watercraft b/c there is a separate column called activity that goes into the specifics\n",
        "    'Unverified': 'Unconfirmed', #Unverified, Under Investigation, and Unconfirmed are the same thing. so consolidate into one category\n",
        "    'Under investigation': 'Unconfirmed',\n",
        "})\n",
        "df[\"Type\"].value_counts()\n",
        "df[\"Type\"] = df[\"Type\"].replace(np.nan,'Missing Data') # Notice the column replacement\n",
        "print(df[\"Type\"].value_counts(), '\\n')\n",
        "df[\"Type\"].unique()\n",
        "\n",
        "#What I noticed that Boat/Boating/Boatomg all were the same and that they can also be categorized as Watercraft.\n",
        "#Since Activity goes more into detail about what happened, I decided that consolidating the overall Type wouldn't be bad.\n",
        "#I also consolidated Unverfieid, Unconfirmed, and Under investigation --> they all seem to be talking about the fact that this is not confirmed yet.\n",
        "#However, I left Invalid and Questionable separate as they are questioning the reliability of the content as opposed to the state of confirmation\n",
        "#I also replaced the nan's into a new title called \"Missing Data\" to help clarify the absence more clearly.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1COtcFnSG0o",
        "outputId": "38bd556a-b39b-4ea6-fc6a-5eda6f4572b0"
      },
      "id": "S1COtcFnSG0o",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unprovoked      4716\n",
            "Provoked         593\n",
            "Invalid          552\n",
            "Watercraft       344\n",
            "Sea Disaster     239\n",
            "Questionable      10\n",
            "Missing Data       5\n",
            "Unconfirmed        3\n",
            "Name: Type, dtype: int64 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Unprovoked', 'Provoked', 'Questionable', 'Watercraft',\n",
              "       'Unconfirmed', 'Invalid', 'Sea Disaster', 'Missing Data'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 2 part 3:\n",
        "url = 'http://www.vcsc.virginia.gov/pretrialdataproject/October%202017%20Cohort_Virginia%20Pretrial%20Data%20Project_Deidentified%20FINAL%20Update_10272021.csv'\n",
        "df = pd.read_csv(url,low_memory=False) # accessing the pretrial data\n",
        "df.head()\n",
        "df = df.rename(columns={\"WhetherDefendantWasReleasedPretrial\": \"Released_Pretrial\"})\n",
        "df[\"Released_Pretrial\"].unique()\n",
        "print(df[\"Released_Pretrial\"].value_counts(), '\\n')\n",
        "df[\"Released_Pretrial\"] = pd.to_numeric(df[\"Released_Pretrial\"], errors='coerce') # Coerce the variable to numeric\n",
        "df[\"Released_Pretrial\"] = df[\"Released_Pretrial\"].replace(9,np.nan)\n",
        "print(df[\"Released_Pretrial\"].value_counts(), '\\n')\n",
        "df[\"Released_Pretrial\"].unique()\n",
        "\n",
        "#For this one, I renamed the column name as it was just very confusing and long to access. I also coerced all the variables to numerics\n",
        "#When I originally checked, there were no missing or nan data. Therefore, based on the criterion, I changed the '9's', that said the data was unclear, to nans.\n",
        "#This makes the data easier to handle."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ooqXudpiDLp",
        "outputId": "0c22d222-8aa0-408c-b7b1-2c2a5e3ce704"
      },
      "id": "7ooqXudpiDLp",
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1    19154\n",
            "0     3801\n",
            "9       31\n",
            "Name: Released_Pretrial, dtype: int64 \n",
            "\n",
            "1.0    19154\n",
            "0.0     3801\n",
            "Name: Released_Pretrial, dtype: int64 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([nan,  0.,  1.])"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.arrays import numeric\n",
        "#Question 2, part 4:\n",
        "url = 'http://www.vcsc.virginia.gov/pretrialdataproject/October%202017%20Cohort_Virginia%20Pretrial%20Data%20Project_Deidentified%20FINAL%20Update_10272021.csv'\n",
        "df = pd.read_csv(url,low_memory=False) # accessing the pretrial data\n",
        "df.head()\n",
        "df = df.rename(columns={\"ImposedSentenceAllChargeInContactEvent\": \"ImposedSentence\",'SentenceTypeAllChargesAtConvictionInContactEvent': 'SentenceType'}) #got too long and confusing to type out\n",
        "df['ImposedSentence'].unique()\n",
        "\n",
        "df['ImposedSentence'] = df['ImposedSentence'].astype(str) #converting to string and taking out commas\n",
        "df['ImposedSentence'] = df[\"ImposedSentence\"].str.replace(',', '')\n",
        "\n",
        "df['SentenceType'].unique() #rechecking for difference\n",
        "\n",
        "df[\"ImposedSentence\"] = df[\"ImposedSentence\"].replace(\" \",np.nan) #replacing the spaces with nans\n",
        "\n",
        "cross_tab = pd.crosstab(df['ImposedSentence'].isnull(),df['SentenceType'])\n",
        "print(cross_tab) #looking at the crosstab of imposed sentence and sentence type\n",
        "\n",
        "df.loc[df['SentenceType'] == 4, 'ImposedSentence'] = 0 #making the values here 0\n",
        "df[\"ImposedSentence\"] = pd.to_numeric(df[\"ImposedSentence\"], errors='coerce') # Coerce the variable to numeric\n",
        "df['ImposedSentence'].value_counts() #rechecking\n",
        "\n",
        "cross_tab = pd.crosstab(df['ImposedSentence'].isnull(),df['SentenceType'])\n",
        "print(cross_tab) #rechecking\n",
        "\n",
        "#Here, I first renamed the column names to be easier to read and use.\n",
        "#Next, I converted all the values into strings and then took out the commas and replaced them with '' (like nothing but now the numbers are more readable)\n",
        "#Then, I replaced all the empty spaces with nans.\n",
        "#Afterwards, I took the crosstab of ImposedSentence and SentenceType and took the \"Other\" of SentenceType and then set them to 0's (as they wouldn't have any imposed sentence)\n",
        "#The 9's I left as nans.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "228gyx28qNtJ",
        "outputId": "d6664352-f6d2-4bbb-d3a9-5f1901fec1d3"
      },
      "id": "228gyx28qNtJ",
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SentenceType        0     1    2     4    9\n",
            "ImposedSentence                            \n",
            "False            8720  4299  914     0    0\n",
            "True                0     0    0  8779  274\n",
            "SentenceType        0     1    2     4    9\n",
            "ImposedSentence                            \n",
            "False            8720  4299  914  8779    0\n",
            "True                0     0    0     0  274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c11bcd96-2834-41a4-80fe-d354b4277fd9",
      "metadata": {
        "id": "c11bcd96-2834-41a4-80fe-d354b4277fd9"
      },
      "source": [
        "**Q3.** This question provides some practice doing exploratory data analysis and visualization.\n",
        "\n",
        "The \"relevant\" variables for this question are:\n",
        "  - `level` - Level of institution (4-year, 2-year)\n",
        "  - `aid_value` - The average amount of student aid going to undergraduate recipients\n",
        "  - `control` - Public, Private not-for-profit, Private for-profit\n",
        "  - `grad_100_value` - percentage of first-time, full-time, degree-seeking undergraduates who complete a degree or certificate program within 100 percent of expected time (bachelor's-seeking group at 4-year institutions)\n",
        "\n",
        "1. Load the `./data/college_completion.csv` data with Pandas.\n",
        "2. What are are the dimensions of the data? How many observations are there? What are the variables included? Use `.head()` to examine the first few rows of data.\n",
        "3. Cross tabulate `control` and `level`. Describe the patterns you see.\n",
        "4. For `grad_100_value`, create a histogram, kernel density plot, boxplot, and statistical description.\n",
        "5. For `grad_100_value`, create a grouped kernel density plot by `control` and by `level`. Describe what you see. Use `groupby` and `.describe` to make grouped calculations of statistical descriptions of `grad_100_value` by `level` and `control`. Which institutions appear to have the best graduation rates?\n",
        "6. Create a new variable, `df['levelXcontrol']=df['level']+', '+df['control']` that interacts level and control. Make a grouped kernel density plot. Which institutions appear to have the best graduation rates?\n",
        "7. Make a kernel density plot of `aid_value`. Notice that your graph is \"bi-modal\", having two little peaks that represent locally most common values. Now group your graph by `level` and `control`. What explains the bi-modal nature of the graph? Use `groupby` and `.describe` to make grouped calculations of statistical descriptions of `aid_value` by `level` and `control`.\n",
        "8. Make a scatterplot of `grad_100_value` by `aid_value`. Describe what you see. Now make the same plot, grouping by `level` and then `control`. Describe what you see. For which kinds of institutions does aid seem to increase graduation rates?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98d34a3b-c21d-4dc9-a8d2-fb7686804ceb",
      "metadata": {
        "id": "98d34a3b-c21d-4dc9-a8d2-fb7686804ceb"
      },
      "source": [
        "**Q4.** This question uses the Airbnb data to practice making visualizations.\n",
        "\n",
        "  1. Load the `./data/airbnb_hw.csv` data with Pandas. You should have cleaned the `Price` variable in question 2, and you'll need it later for this question.\n",
        "  2. What are are the dimensions of the data? How many observations are there? What are the variables included? Use `.head()` to examine the first few rows of data.\n",
        "  3. Cross tabulate `Room Type` and `Property Type`. What patterns do you see in what kinds of rentals are available? For which kinds of properties are private rooms more common than renting the entire property?\n",
        "  4. For `Price`, make a histogram, kernel density, box plot, and a statistical description of the variable. Are the data badly scaled? Are there many outliers? Use `log` to transform price into a new variable, `price_log`, and take these steps again.\n",
        "  5. Make a scatterplot of `price_log` and `Beds`. Describe what you see. Use `.groupby()` to compute a desciption of `Price` conditional on/grouped by the number of beds. Describe any patterns you see in the average price and standard deviation in prices.\n",
        "  6. Make a scatterplot of `price_log` and `Beds`, but color the graph by `Room Type` and `Property Type`. What patterns do you see? Compute a description of `Price` conditional on `Room Type` and `Property Type`. Which Room Type and Property Type have the highest prices on average? Which have the highest standard deviation? Does the mean or median appear to be a more reliable estimate of central tendency, and explain why?\n",
        "  7. We've looked a bit at this `price_log` and `Beds` scatterplot. Use seaborn to make a `jointplot` with `kind=hex`. Where are the data actually distributed? How does it affect the way you think about the plots in 5 and 6?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5",
      "metadata": {
        "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5"
      },
      "source": [
        "**Q5.** Many important datasets contain a race variable, typically limited to a handful of values often including Black, White, Asian, Latino, and Indigenous. This question looks at data gathering efforts on this variable by the U.S. Federal government.\n",
        "\n",
        "1. How did the most recent US Census gather data on race?\n",
        "\n",
        "The most recent US Census was conducted in 2020. They gathered data of race by asking particpants to self identify. There were two main questions, asking if participants were of Hispanic/Spanish/Latino origin and if not to select the box(es) which applied to their race. These were the categories that were asked: White, Black or African American, American Indian or Alaska Native, Asian, and Native Hawaiian or Other Pacific Islander.\n",
        "2. Why do we gather these data? What role do these kinds of data play in politics and society? Why does data quality matter?\n",
        "\n",
        "We gather data on race to help our government make policies that are more reflective of our population. By accumulating accurate information of citizens and non-citizens, governments, either state or national, can look at the current systems in place and see if they are disadvantaging any population. To further elaborate, state governments can use this data when they are redistricting to better reflect population changes and/or national governments can take this data and see if current healthcare systems are instituting policies that are disenfranchising a certain race. Collecting information of race is an important factor to help our government make sure that civil rights laws our being properly upheld. In terms of society, collecting information about race can help us look at popular trends and practices to see how reflective they are of the population. It can help us study patterns of change and societal trends. This in turn can help our government officials make appropriate policy decisions as well.\n",
        "\n",
        "Data quality matters because we want to be able to produce analyses that are accurate and useful. If the quality of our data is poor (i.e., contains bias, poor sampling, etc.) then our analysis becomes unreliable. Then we cannot use this to make inferences or use this as a bases to adjust policies as we will end up with creating even more disparities, unfair allocation of resources, or ineffective/harmful policies.\n",
        "\n",
        "3. Please provide a constructive criticism of how the Census was conducted: What was done well? What do you think was missing? How should future large scale surveys be adjusted to best reflect the diversity of the population? Could some of the Census' good practices be adopted more widely to gather richer and more useful data?\n",
        "\n",
        "This Census took place right in the height of the Coronavirus pandemic. I think what was done well was the implementation of online surveys alongside the telephone and paper forms. The online forms, at such a time, would have incentivized more people to fill out the form (easy to access and quick submission). In addition, this Census also allowed people to take it in multiple different languages - this accounted for the diverse population of our country. Adding on, the government took steps to also reach individuals who were living in remote, transient, or group quarters, which tried to minimize sampling bias. And last but not the least, they employed many advertisement and outreach campaigns to educate/incentivize people to respond.\n",
        "\n",
        "While that being said, there were still a few hiccups. For instance, the Census grossly undercounted the Hispanic population, African Americans, and Native Americans while overcounting Asians and White Non-Hispanics. Children were the most undercounted group than any other age group. Homeowners were overcounted whereas renters were undercounted. Thus the quality of the data acquired is unreliable and still upto dispute. With the COVID-19 pandemic throwing many restrictions their way, there were still many areas that were not accounted for. And finally, the race and ethnicity 2-question format doesn't accurately reflect/represent the diverse population. For instance, having 1 box for Asians doesn't adequetly describe the population (East Asians are different from South Asians who are different from South East Asians).\n",
        "\n",
        "Future large scale surveys could be adjusted to further diversify the options for race. They could include further options (like a tiered response like Asian, and then selecting which kind of Asian) or include open ended responses for patients to describe their racial identity. In addition, larger scale surveys need to be adjusted to be able to account for unforseen disruptions (like this pandemic). One potential cause of error for such disproportionate rates was the reliance of GPS and other map tools to discern which households were empty/occupied. By taking into consideration that technology may not always be reliable and employing each state to be in charge of keeping track of households, there can be a more streamlined approach in collecting responses. In addition, states can test out smaller-scale surveys to test out newer practices/questions planned to be added on to the census. This can both help mitigate any errors that may occur and also help them be aware of how to mitigate over/undercounting populatinos.\n",
        "\n",
        "And yes, the good practices of the Census should be adopted. Their inclusion of an online platform is great - noting how they didn't just rely on this method and employed telephone as well as in person visits is important in eliminating sampling bias. Employing practices that are reflective of today's technology can better reach people and obtain accurate results. The Census also kept in mind and prioritized reaching hard-to-reach populations. Building on what they have done and using their approaches as a baseline would provide for richer data. Their advertising (with grassroot leaders and influential individuals) to fill out the Census is pretty crucial to build trust and willingness to participate as well.\n",
        "\n",
        "4. How did the Census gather data on sex and gender? Please provide a similar constructive criticism of their practices.\n",
        "\n",
        "The Census gathered data on sex and gender was a single questions asking people to state whether the people of their household were male or female (asking for their biological sex rather than their gender). The Census Bureau thought it would be more effective to get information about their biological sex as opposed to their gender.\n",
        "\n",
        "In terms of what went well, the question was simple - the binary aspect allowed for easier data collection. Also, this question was consistent to the ones before. Therefore, it makes it easier to compare trends over time.\n",
        "\n",
        "In terms of what was missing, it doesn't account for individuals who are intersex or who may not strictly identify as male or female. Not considering their gender, in which has been associated to be increasingly different from sex, is not a holistic understanding of a person's identity. In addition, not recognizing individuals who may be non-binary or genderqueer could have drastic consequences: the lack of representation can be overlooked in policy creation and resource allocation which can cause this proportion of individuals to be disenfranchized.\n",
        "\n",
        "For future large-scale studies, there can be an option for individuals to select if they are non-binary/gender queer. Also, there should be two separate questions: one for sex and the other for gender. This will better account for individuals who are trans or gender queer and generate policies that are reflective of their composition. In addition, since the difference between sex and gender may not be clear for everyone, there should be some context and clarity given to eliminate any confusion and any potential errors that could be a result of that.\n",
        "\n",
        "The Census' clear and simple wording of the questions can be adopted to the changes I've stated for future large-scale studies. Having the simplicity can enable all participants to answer to the best of their abilities. Also, utilizing the basis of consistency (incorporating the same questions, terminology, etc. to the best of their abilities) can better help analyses see the trends over time.\n",
        "\n",
        "5. When it comes to cleaning data, what concerns do you have about protected characteristics like sex, gender, sexual identity, or race? What challenges can you imagine arising when there are missing values? What good or bad practices might people adopt, and why?\n",
        "\n",
        "In terms of concerns, one of my biggest ones will be how confidential the acquired data will be. It is very important that the protected characterstics stay confidential and they are not handled inappropriately. Another concern I have when cleaning data is losing important specifics by combining \"similar\" data into groups. Like I've mentioned before, if we were to combine race into larger categories we would lose important parts of certain individual's identity which may not be adequately satisfied by lumping them into a larger group. But we also don't want too many specifics being riddled within the data that we lose important insights about overall trends. Therefore, there needs to be an appropriate balance in over/under simplification when cleaning. We don't also want go give way to bias or discrimination in our analysis if these protected charactersitics are not cleaned correctly.\n",
        "\n",
        "I can imagine bias arising when there are missing values. Like in the category we discussed above, the lack of inclusion of different sexes/genders can arise for an oversight in terms of the actual makeup of the population. This can also lead to another challenge of making wrong assumptions or analyses that can give rise to policies or systemes that may disadvantage certain groups. Missing data may also lead to another challenge of not being accurately reflective of our population - in which subsequent analsysis may decrease the quality of the data and the work being done with it as a whole.\n",
        "\n",
        "In terms of good practices, people may tend to start cleaning data in a copy rather than the original dataset. This way if any errors/biases seem to have arised, they always have the original to restart their work. They may also loop in other perspectives (i.e., maybe representatives of a group that they are working with) to evaluate how they are handling the data. In addition, they may also work on handling these sensitive data with more care (i.e., making sure to delete any identifiers linking the data to a particular person,prioritizing fairness/accurate representation, etc.). This is to further ensure that sensitive information stays private and doesn't itself influence any bias on the data analysis portion. However, in terms of bad practices, people may tend to lump distinct categories into a broad overarching category - this overgeneralization and oversimplification may be due to having neater graphs or a need for simplicity. People may tend to ignore missing data and continue their analysis without them. This is bad in the sense that this can lead to biased analysis. Or people may arbitrarily fill in the spots of missing data with other characteristics that may or may not have an rationale. This will make it harder to do the next stages, such as visualization.\n",
        "\n",
        "6. Suppose someone invented an algorithm to impute values for protected characteristics like race, gender, sex, or sexuality. What kinds of concerns would you have?\n",
        "\n",
        "I would be concerned about ethics. Assigning these protected charactersitics, whether or not they may be true, is dismissive of one's agency and is blatantly wrong. While there may be a chance this could be true, there is an equal if not higher chance the imputed value could be wrong. This can severely skew the data and provide incorrect analyses. Since algorithms learn from existing data, if the current dataset we are working with is wrong, then future algorithms could take this data and could be trained incorrectly. This could lead to the perpetuation of said biases and cause wider divides. These protected characteristics, especially something like sexuality, can be fluid and change over time. There is no way to then validate if the algorithm is performing correctly due to the nature of these characteristics."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f38f2fd-6381-481d-bba9-017f3d363426",
      "metadata": {
        "id": "2f38f2fd-6381-481d-bba9-017f3d363426"
      },
      "source": [
        "**Q6.** Open the `./data/CBO_data.pdf` file. This contains tax data for 2019, explaining where the money comes from that the U.S. Federal Government Spends in terms of taxation on individuals/families and payroll taxes (the amount that your employer pays in taxes on your wages).\n",
        "\n",
        "For some context, the Federal government ultimately spent about $4.4 trillion in 2019, which was 21% of GDP (the total monetary value of all goods and services produced within the United States). Individual Income Taxes is the amount individuals pay on their wages to the Federal government, Corporate Income Taxes is the taxes individuals pay on capital gains from investment when they sell stock or other financial instruments, Payroll Taxes is the tax your employer pays on your wages, Excises and Customs Duties are taxes on goods or services like sin taxes on cigarettes or alcohol, and Estate and Gift Taxes are taxes paid on transfers of wealth to other people.\n",
        "\n",
        "1. Get the Millions of Families and Billions of Dollars data into a .csv file and load it with Pandas.\n",
        "2. Create a bar plot of individual income taxes by income decile. Explain what the graph shows. Why are some values negative?\n",
        "3. Create a bar plot of Total Federal Taxes by income decile. Which deciles are paying net positive amounts, and which are paying net negative amounts?\n",
        "4. Create a stacked bar plot for which Total Federal Taxes is grouped by Individual Income Taxes, Payroll Taxes, Excises and Customs Duties, and Estate and Gift Taxes. How does the share of taxes paid vary across the adjusted income deciles? (Hint: Are these the kind of data you want to melt?)\n",
        "5. Below the Total line for Millions of Families and Billions of Dollars, there are data for the richest of the richest families. Plot this alongside the bars for the deciles above the Total line. Describe your results.\n",
        "6. Get the Percent Distribution data into a .csv file and load it with Pandas. Create a bar graph of Total Federal Taxes by income decile.\n",
        "7. A tax system is progressive if higher-income and wealthier individuals pay more than lower-income and less wealthy individuals, and it is regressive if the opposite is true. Is the U.S. tax system progressive in terms of amount paid? In terms of the percentage of the overall total?\n",
        "8. Do the rich pay enough in taxes? Defend your answer."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}